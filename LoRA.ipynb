{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting random seed\n",
    "_=torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the MNIST dataset --> Applying transforms --> shifting to cuda/mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "## loading the dataset\n",
    "mnist_dataset=datasets.MNIST(root='/Users/praroopchanda/Desktop/Models_Coding_Practice/LoRA',train=True,download=True,transform=transform)\n",
    "## creating a dataloader for training\n",
    "train_loader=DataLoader(mnist_dataset,batch_size=10,shuffle=True)\n",
    "\n",
    "#MNIST Test set\n",
    "mnist_testset=datasets.MNIST(root=\"/Users/praroopchanda/Desktop/Models_Coding_Practice/LoRA\",train=False,download=True,transform=transform)\n",
    "test_loader=DataLoader(mnist_testset,batch_size=10,shuffle=True)\n",
    "\n",
    "# device\n",
    "device=torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating overly expensive neural network\n",
    "## in this passing image as by flattening it\n",
    "\n",
    "class ExpensiveNet(nn.Module):\n",
    "    def __init__(self,hidden_size_1=1000,hidden_size_2=2000):\n",
    "        super().__init__()\n",
    "        self.linear_1=nn.Linear(28*28,hidden_size_1)\n",
    "        self.linear_2=nn.Linear(hidden_size_1,hidden_size_2)\n",
    "        self.linear_3=nn.Linear(hidden_size_2,10)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self,img):\n",
    "        x=img.view(-1,28*28) ## flattening the image tensor\n",
    "        x=self.relu(self.linear_1(x))\n",
    "        x=self.relu(self.linear_2(x))\n",
    "        x=self.linear_3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "## moving to device --> mps in our case\n",
    "net=ExpensiveNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training just for one Epoch -->one pass of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:38<00:00, 156.75it/s, loss=0.236]\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader:DataLoader,net,epochs=5,total_iteration_limit=None):\n",
    "    cross_el=nn.CrossEntropyLoss() ## defining the loss\n",
    "    optimizer=torch.optim.Adam(net.parameters(),lr=0.001) ## defining the optimizer\n",
    "\n",
    "    total_iterations=0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum=0\n",
    "        num_iterations=0\n",
    "\n",
    "        data_iterator=tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iteration_limit is not None:\n",
    "            data_iterator.total=total_iteration_limit\n",
    "\n",
    "        for data in data_iterator:\n",
    "            num_iterations+=1\n",
    "            total_iterations+=1\n",
    "\n",
    "            x,y=data\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output=net(x.view(-1,28*28))  ## i think we should be able to do it directly as well --> as it is changing the dimensions in the forward method\n",
    "            loss=cross_el(output,y)\n",
    "\n",
    "            loss_sum+=loss.item()\n",
    "            avg_loss=loss_sum/num_iterations\n",
    "\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iteration_limit is not None and total_iterations>=total_iteration_limit:\n",
    "                return\n",
    "\n",
    "\n",
    "train(train_loader,net,epochs=1)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keeping a copy of original weights(cloning them) to check later for modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights={}\n",
    "for name,param in net.named_parameters():\n",
    "    original_weights[name]=param.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 253.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954\n",
      "wrong count for 0 is 18\n",
      "wrong count for 1 is 30\n",
      "wrong count for 2 is 52\n",
      "wrong count for 3 is 119\n",
      "wrong count for 4 is 25\n",
      "wrong count for 5 is 11\n",
      "wrong count for 6 is 57\n",
      "wrong count for 7 is 56\n",
      "wrong count for 8 is 18\n",
      "wrong count for 9 is 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    correct=0\n",
    "    total=0\n",
    "\n",
    "    wrong_counts=[0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader,desc=\"Testing\"):\n",
    "            x,y=data\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            output=net(x.view(-1,28*28))\n",
    "            \n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i)==y[idx]: ## matching the indices\n",
    "                    correct+=1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]]+=1\n",
    "                total+=1\n",
    "\n",
    "    print(f\"Accuracy: {round(correct/total,3)}\")\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f\"wrong count for {i} is {wrong_counts[i]}\")                \n",
    "\n",
    "test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe that accuracy of digit 3 is not very good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking total number of parameters in the original layers first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1, W: 784000 + B: 1000\n",
      "Layer: 2, W: 2000000 + B: 2000\n",
      "Layer: 3, W: 20000 + B: 10\n",
      "Total number of parameters are: 2807010\n",
      "2813804\n"
     ]
    }
   ],
   "source": [
    "### Total parameter count including all layers\n",
    "total_parameters_original=0\n",
    "for index, layer in enumerate([net.linear_1,net.linear_2,net.linear_3]):\n",
    "    total_parameters_original+=layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f\"Layer: {index+1}, W: {layer.weight.nelement()} + B: {layer.bias.nelement()}\")\n",
    "print(f\"Total number of parameters are:\",total_parameters_original)    \n",
    "\n",
    "## Can also directly do by parameter count directly from the model\n",
    "params_count=0\n",
    "\n",
    "for params in net.parameters(): \n",
    "    params_count+=params.numel()\n",
    "\n",
    "print(params_count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LORA Parametrization from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self,features_in: int, features_out:int, rank:int=1,alpha:int =1,device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        ## section 4.1 of the paper\n",
    "        ## random gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning\n",
    "        self.lora_A=nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
    "        self.lora_B=nn.Parameter(torch.zeros((features_in,rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A,mean=0,std=1) ## random gaussian initialization\n",
    "\n",
    "        ## Section 4.1 of the paper:\n",
    "        # Scaling ∆W by α/r , where α is a constant --> this will basically help preventing instability and large updates in training\n",
    "        # when optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately\n",
    "        # As a result, we just simply set α to first r and do not tune it\n",
    "        # This scaling will also help reduce the effect when we increase r as its (α/r) , so reduce the need to retune hyperparameters as r is varied.\n",
    "\n",
    "        self.scale=alpha/rank\n",
    "\n",
    "        self.enabled=True\n",
    "\n",
    "    def forward(self,original_weights):\n",
    "        if self.enabled:\n",
    "            ## return X+(B@A)*scale\n",
    "            return original_weights+(self.lora_B @ self.lora_A).view(original_weights.shape)*self.scale\n",
    "            ## can also do without view as it would not give error ---> return original_weights+torch.matmul(self.lora_B,self.lora_A).view(originak_weights.shape)*self.scale\n",
    "        else:\n",
    "            return original_weights    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pytorch Parametrization to inject/replace original weights with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def linear_layer_parametrization(layer, device,rank=1,lora_alpha=1):\n",
    "    '''\n",
    "    ** Only adding paramterization to the weights and leave the bias\n",
    "    ** The study limits to only adapting weights to the downstream tasks, and freeze the MLP modules\n",
    "    '''\n",
    "\n",
    "    features_in, features_out=layer.weight.shape\n",
    "\n",
    "    return LoRAParametrization(features_in, features_out,rank,lora_alpha,device)\n",
    "\n",
    "'''\n",
    "Registering LoRA now\n",
    "'''\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear_1,\"weight\",linear_layer_parametrization(net.linear_1,device) ## this basically replaces net.linear1 with now LoRAParametrization() --> Tensor which has the same output\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear_2,\"weight\",linear_layer_parametrization(net.linear_2,device)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear_3,\"weight\",linear_layer_parametrization(net.linear_3,device)\n",
    ")\n",
    "\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [net.linear_1,net.linear_2,net.linear_3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled=enabled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying total number of parameters added by LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0262,  0.0457, -0.0029,  ...,  0.0485,  0.0302,  0.0286],\n",
      "        [ 0.0026,  0.0074,  0.0120,  ...,  0.0021,  0.0164, -0.0075],\n",
      "        [ 0.0110,  0.0461, -0.0021,  ...,  0.0109,  0.0324,  0.0392],\n",
      "        ...,\n",
      "        [ 0.0099,  0.0735,  0.0718,  ...,  0.0407,  0.0669,  0.0143],\n",
      "        [ 0.0531,  0.0164,  0.0022,  ...,  0.0365,  0.0300,  0.0308],\n",
      "        [ 0.0151, -0.0086,  0.0426,  ...,  0.0519,  0.0254,  0.0358]],\n",
      "       device='mps:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "print(net.linear_1.weight)\n",
    "print(net.linear_1.parametrizations[\"weight\"][0].lora_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:1: W: torch.Size([1000, 784])+ B:torch.Size([1000])+ lora_A:torch.Size([1, 784]) + lora_B: torch.Size([1000, 1]) \n",
      "Layer:2: W: torch.Size([2000, 1000])+ B:torch.Size([2000])+ lora_A:torch.Size([1, 1000]) + lora_B: torch.Size([2000, 1]) \n",
      "Layer:3: W: torch.Size([10, 2000])+ B:torch.Size([10])+ lora_A:torch.Size([1, 2000]) + lora_B: torch.Size([10, 1]) \n",
      "Total number of parameters (original): 2807010\n",
      "Total number of parameters (original+LoRA): 2813804\n",
      " number of parameters induced by LoRA): 6794\n",
      "Parameters increment:0.242%\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora=0\n",
    "total_parameters_non_lora=0\n",
    "for index,layer in enumerate([net.linear_1,net.linear_2,net.linear_3]):\n",
    "    total_parameters_lora+=layer.parametrizations['weight'][0].lora_A.nelement() + layer.parametrizations['weight'][0].lora_B.nelement()\n",
    "    total_parameters_non_lora+=layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(\n",
    "        f'Layer:{index+1}: W: {layer.weight.shape}+ B:{layer.bias.shape}+ lora_A:{layer.parametrizations['weight'][0].lora_A.shape} + lora_B: {layer.parametrizations['weight'][0].lora_B.shape} '\n",
    "    )     \n",
    "\n",
    "\n",
    "## Non Lora parameters should be equal to original network\n",
    "assert total_parameters_non_lora== total_parameters_original\n",
    "\n",
    "print(f\"Total number of parameters (original):\",total_parameters_non_lora)\n",
    "print(f\"Total number of parameters (original+LoRA):\",total_parameters_lora+total_parameters_non_lora)\n",
    "print(f\" number of parameters induced by LoRA):\",total_parameters_lora)\n",
    "parameters_increment=(total_parameters_lora/total_parameters_non_lora)*100\n",
    "\n",
    "print(f\"Parameters increment:{parameters_increment:.3f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_count=0\n",
    "# for params in net.parameters(): \n",
    "#     params_count+=params.numel()\n",
    "\n",
    "# print(params_count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now fine tuning only LORA weights for digit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing non LoRA paramater:linear_1.bias\n",
      "freezing non LoRA paramater:linear_1.parametrizations.weight.original\n",
      "freezing non LoRA paramater:linear_2.bias\n",
      "freezing non LoRA paramater:linear_2.parametrizations.weight.original\n",
      "freezing non LoRA paramater:linear_3.bias\n",
      "freezing non LoRA paramater:linear_3.parametrizations.weight.original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 99/100 [00:02<00:00, 46.30it/s, loss=0.164]\n"
     ]
    }
   ],
   "source": [
    " ## freezing non-lora parameters\n",
    "for name, param in net.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        param.requires_grad=False\n",
    "        print(f\"freezing non LoRA paramater:{name}\")\n",
    "\n",
    "## Loading MNIST dataset again, by keeping only the digit 9\n",
    "mnist_trainset=datasets.MNIST(root=\"/Users/praroopchanda/Desktop/Models_Coding_Practice/LoRA\",train=True,transform=transform,download=True)\n",
    "exlude_indices=mnist_trainset.targets==3\n",
    "mnist_trainset.data=mnist_trainset.data[exlude_indices]\n",
    "mnist_trainset.targets=mnist_trainset.targets[exlude_indices]\n",
    "\n",
    "## create a dataloader for training\n",
    "train_finetune_loader=torch.utils.data.DataLoader(mnist_trainset,batch_size=10, shuffle=True)\n",
    "\n",
    "## Train/Fine tune the network now using LoRA and doing it for just 100 batches assumption being it would improve performance\n",
    "train(train_finetune_loader,net,epochs=1,total_iteration_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that we only changed the LoRA parameters and not the original ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### comparing the frozen paramters to orignal weights \n",
    "assert torch.all(net.linear_1.parametrizations.weight.original==original_weights['linear_1.weight'])\n",
    "assert torch.all(net.linear_2.parametrizations.weight.original==original_weights['linear_2.weight'])\n",
    "assert torch.all(net.linear_3.parametrizations.weight.original==original_weights['linear_3.weight'])\n",
    "\n",
    "## now comparing the paramters of full lora and doing matrix multiplication by ourselves\n",
    "## should be same\n",
    "enable_disable_lora(enabled=True)\n",
    "\n",
    "assert torch.equal(net.linear_1.weight,net.linear_1.parametrizations.weight.original+(net.linear_1.parametrizations.weight[0].lora_B @ net.linear_1.parametrizations.weight[0].lora_A)** net.linear_1.parametrizations.weight[0].scale)\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "assert torch.equal(net.linear_1.weight,original_weights['linear_1.weight'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network now (should be a better classifier on digit 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:06<00:00, 161.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.926\n",
      "wrong count for 0 is 29\n",
      "wrong count for 1 is 38\n",
      "wrong count for 2 is 135\n",
      "wrong count for 3 is 22\n",
      "wrong count for 4 is 27\n",
      "wrong count for 5 is 47\n",
      "wrong count for 6 is 64\n",
      "wrong count for 7 is 141\n",
      "wrong count for 8 is 143\n",
      "wrong count for 9 is 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with LoRA enabled\n",
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:04<00:00, 233.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954\n",
      "wrong count for 0 is 18\n",
      "wrong count for 1 is 30\n",
      "wrong count for 2 is 52\n",
      "wrong count for 3 is 119\n",
      "wrong count for 4 is 25\n",
      "wrong count for 5 is 11\n",
      "wrong count for 6 is 57\n",
      "wrong count for 7 is 56\n",
      "wrong count for 8 is 18\n",
      "wrong count for 9 is 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with LoRA disabled (Same accuracy as before)\n",
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
